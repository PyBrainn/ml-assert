{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ml-assert","text":"<p>A lightweight, chainable assertion toolkit for validating data and models in ML workflows.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Data Validation: Schema, nulls, uniqueness, ranges, and value sets.</li> <li>Statistical Checks: Distribution drift (KS, Chi-square) and drift detection.</li> <li>Model Performance: Accuracy, precision, recall, F1-score, and ROC AUC.</li> <li>Fairness &amp; Explainability: Demographic parity, equal opportunity, and SHAP values.</li> <li>Integrations: MLflow, Prometheus, Slack, and DVC.</li> <li>Fluent Interface: Chain assertions for clean, readable code.</li> <li>CLI Runner: Run checks from a YAML configuration.</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    A[Core Assertions] --&gt; B[DataFrameAssertion]\n    A --&gt; C[ModelAssertion]\n    A --&gt; D[FairnessAssertion]\n    B &amp; C &amp; D --&gt; E[AssertionResult]\n    E --&gt; F[Integrations]\n    F --&gt; G[SlackAlerter]\n    F --&gt; H[PrometheusExporter]\n    F --&gt; I[MLflowLogger]\n    F --&gt; J[DVCArtifactChecker]\n    E --&gt; K[Plugins]\n    K --&gt; L[Custom Plugins]\n    K --&gt; M[Built-in Plugins]\n    E --&gt; N[CLI Runner]\n    N --&gt; O[YAML Config]\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>User Guide: High-level documentation with examples and use cases</li> <li>Data Assertions</li> <li>Statistical Assertions</li> <li>Model Performance</li> <li>Fairness &amp; Explainability</li> <li>Integrations</li> <li> <p>Plugins</p> </li> <li> <p>API Reference: Detailed technical documentation</p> </li> <li>Core API</li> <li>Data API</li> <li>Model API</li> <li>Stats API</li> <li>Fairness API</li> <li>Integrations API</li> <li>Plugins API</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ml-assert\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#data-assertions","title":"Data Assertions","text":"<pre><code>import pandas as pd\nfrom ml_assert import Assertion, schema\n\ndf = pd.DataFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"A\", \"B\", \"C\"],\n    \"score\": [0.9, 0.8, 0.7]\n})\n\n# Create a schema\ns = schema()\ns.col(\"id\").is_unique()\ns.col(\"score\").in_range(0.0, 1.0)\n\n# Validate\nAssertion(df).satisfies(s).no_nulls().validate()\n</code></pre>"},{"location":"#model-performance","title":"Model Performance","text":"<pre><code>from ml_assert import assert_model\n\nassert_model(y_true, y_pred, y_scores) \\\n    .accuracy(min_score=0.8) \\\n    .precision(min_score=0.8) \\\n    .recall(min_score=0.8) \\\n    .f1(min_score=0.8) \\\n    .roc_auc(min_score=0.9) \\\n    .validate()\n</code></pre>"},{"location":"#statistical-drift","title":"Statistical Drift","text":"<pre><code>from ml_assert.stats.drift import assert_no_drift\n\nassert_no_drift(df_train, df_test, alpha=0.05)\n</code></pre>"},{"location":"#fairness-explainability","title":"Fairness &amp; Explainability","text":"<pre><code>from ml_assert.fairness import assert_fairness\n\nassert_fairness(\n    y_true=y_true,\n    y_pred=y_pred,\n    sensitive_features=sensitive_features,\n    metrics=[\"demographic_parity\", \"equal_opportunity\"],\n    threshold=0.1\n)\n</code></pre>"},{"location":"#cli-usage","title":"CLI Usage","text":"<p>Run checks from a YAML file:</p> <pre><code>ml_assert run --config /path/to/config.yaml\n</code></pre> <p>Example config:</p> <pre><code>steps:\n  - type: drift\n    train: 'ref.csv'\n    test: 'cur.csv'\n    alpha: 0.05\n\n  - type: model_performance\n    y_true: 'y_true.csv'\n    y_pred: 'y_pred.csv'\n    y_scores: 'y_scores.csv'\n    assertions:\n      accuracy: 0.75\n      roc_auc: 0.80\n\n  - type: file_exists\n    path: 'my_model.pkl'\n</code></pre> <p>For more examples and detailed documentation, see the User Guide and API Reference.</p>"},{"location":"SECURITY/","title":"Security","text":""},{"location":"SECURITY/#security-contact-information","title":"Security contact information","text":"<p>To report a security vulnerability, please use the Tidelift security contact. Tidelift will coordinate the fix and disclosure.</p>"},{"location":"cross_validation/","title":"Cross-Validation","text":"<p>ML-Assert provides robust cross-validation support for evaluating machine learning models. This feature allows you to perform comprehensive model evaluation using various cross-validation strategies and metrics.</p>"},{"location":"cross_validation/#overview","title":"Overview","text":"<p>Cross-validation is a crucial technique for assessing model performance and generalization ability. ML-Assert implements several cross-validation strategies and provides easy-to-use assertion functions for various metrics.</p>"},{"location":"cross_validation/#available-cross-validation-strategies","title":"Available Cross-Validation Strategies","text":""},{"location":"cross_validation/#k-fold-cross-validation","title":"K-Fold Cross-Validation","text":"<p>The standard k-fold cross-validation splits the data into k equal parts and uses each part as a validation set while training on the remaining k-1 parts.</p> <pre><code>from ml_assert.model.cross_validation import assert_cv_accuracy_score\n\n# Using 5-fold cross-validation\nassert_cv_accuracy_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.85,\n    cv_type='kfold',\n    n_splits=5\n)\n</code></pre>"},{"location":"cross_validation/#stratified-k-fold-cross-validation","title":"Stratified K-Fold Cross-Validation","text":"<p>Stratified k-fold cross-validation maintains the class distribution in each fold, which is particularly useful for imbalanced datasets.</p> <pre><code># Using stratified 5-fold cross-validation\nassert_cv_accuracy_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.85,\n    cv_type='stratified',\n    n_splits=5\n)\n</code></pre>"},{"location":"cross_validation/#leave-one-out-cross-validation","title":"Leave-One-Out Cross-Validation","text":"<p>Leave-One-Out (LOO) cross-validation uses each sample as a validation set while training on all other samples. This is useful for small datasets.</p> <pre><code># Using leave-one-out cross-validation\nassert_cv_accuracy_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.85,\n    cv_type='loo'\n)\n</code></pre>"},{"location":"cross_validation/#available-metrics","title":"Available Metrics","text":"<p>ML-Assert supports various metrics for cross-validation evaluation:</p>"},{"location":"cross_validation/#accuracy-score","title":"Accuracy Score","text":"<pre><code>from ml_assert.model.cross_validation import assert_cv_accuracy_score\n\nassert_cv_accuracy_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.85\n)\n</code></pre>"},{"location":"cross_validation/#precision-score","title":"Precision Score","text":"<pre><code>from ml_assert.model.cross_validation import assert_cv_precision_score\n\nassert_cv_precision_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.80\n)\n</code></pre>"},{"location":"cross_validation/#recall-score","title":"Recall Score","text":"<pre><code>from ml_assert.model.cross_validation import assert_cv_recall_score\n\nassert_cv_recall_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.80\n)\n</code></pre>"},{"location":"cross_validation/#f1-score","title":"F1 Score","text":"<pre><code>from ml_assert.model.cross_validation import assert_cv_f1_score\n\nassert_cv_f1_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.80\n)\n</code></pre>"},{"location":"cross_validation/#roc-auc-score","title":"ROC AUC Score","text":"<pre><code>from ml_assert.model.cross_validation import assert_cv_roc_auc_score\n\nassert_cv_roc_auc_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.80\n)\n</code></pre>"},{"location":"cross_validation/#getting-cross-validation-summary","title":"Getting Cross-Validation Summary","text":"<p>You can get a comprehensive summary of all metrics across cross-validation folds:</p> <pre><code>from ml_assert.model.cross_validation import get_cv_summary\n\nsummary = get_cv_summary(\n    model=my_model,\n    X=X,\n    y=y,\n    cv_type='kfold',\n    n_splits=5\n)\n\nprint(summary)\n</code></pre> <p>The summary includes: - Mean score for each metric - Standard deviation of scores - Minimum score across folds - Maximum score across folds</p>"},{"location":"cross_validation/#advanced-usage","title":"Advanced Usage","text":""},{"location":"cross_validation/#parallel-processing","title":"Parallel Processing","text":"<p>Cross-validation computations are automatically parallelized using all available CPU cores for faster evaluation.</p>"},{"location":"cross_validation/#custom-cross-validation","title":"Custom Cross-Validation","text":"<p>You can use any scikit-learn compatible cross-validation splitter by passing it directly to the assertion functions.</p>"},{"location":"cross_validation/#error-handling","title":"Error Handling","text":"<p>The cross-validation module includes comprehensive error handling for: - Invalid model types - Incompatible data types - Invalid cross-validation parameters - Computation errors</p>"},{"location":"cross_validation/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Strategy:</li> <li>Use k-fold for balanced datasets</li> <li>Use stratified k-fold for imbalanced datasets</li> <li> <p>Use leave-one-out for small datasets</p> </li> <li> <p>Set Appropriate Thresholds:</p> </li> <li>Consider the problem domain</li> <li>Account for class imbalance</li> <li> <p>Use multiple metrics for comprehensive evaluation</p> </li> <li> <p>Monitor Performance:</p> </li> <li>Check standard deviation across folds</li> <li>Look for consistent performance</li> <li>Investigate high variance in scores</li> </ol>"},{"location":"cross_validation/#examples","title":"Examples","text":""},{"location":"cross_validation/#complete-model-evaluation","title":"Complete Model Evaluation","text":"<pre><code>from ml_assert.model.cross_validation import (\n    assert_cv_accuracy_score,\n    assert_cv_precision_score,\n    assert_cv_recall_score,\n    assert_cv_f1_score,\n    assert_cv_roc_auc_score,\n    get_cv_summary\n)\n\n# Evaluate model with multiple metrics\nassert_cv_accuracy_score(model, X, y, min_score=0.85)\nassert_cv_precision_score(model, X, y, min_score=0.80)\nassert_cv_recall_score(model, X, y, min_score=0.80)\nassert_cv_f1_score(model, X, y, min_score=0.80)\nassert_cv_roc_auc_score(model, X, y, min_score=0.80)\n\n# Get detailed summary\nsummary = get_cv_summary(model, X, y)\nprint(\"Model Performance Summary:\")\nfor metric, stats in summary.items():\n    if stats is not None:\n        print(f\"\\n{metric.upper()}:\")\n        print(f\"  Mean: {stats['mean']:.4f}\")\n        print(f\"  Std:  {stats['std']:.4f}\")\n        print(f\"  Min:  {stats['min']:.4f}\")\n        print(f\"  Max:  {stats['max']:.4f}\")\n</code></pre>"},{"location":"cross_validation/#handling-imbalanced-data","title":"Handling Imbalanced Data","text":"<pre><code># Using stratified cross-validation for imbalanced data\nassert_cv_f1_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.75,\n    cv_type='stratified',\n    n_splits=5\n)\n</code></pre>"},{"location":"cross_validation/#small-dataset-evaluation","title":"Small Dataset Evaluation","text":"<pre><code># Using leave-one-out cross-validation for small dataset\nassert_cv_accuracy_score(\n    model=my_model,\n    X=X,\n    y=y,\n    min_score=0.80,\n    cv_type='loo'\n)\n</code></pre>"},{"location":"data_assertions/","title":"Data Assertions","text":"<p>The <code>ml_assert</code> module provides a fluent, chainable API for validating the integrity and structure of your <code>pandas</code> DataFrames.</p>"},{"location":"data_assertions/#quick-start","title":"Quick Start","text":"<pre><code>import pandas as pd\nfrom ml_assert import Assertion, schema\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"age\": [25, 30, 35],\n    \"plan_type\": [\"basic\", \"premium\", \"basic\"]\n})\n\n# Create a schema\ns = schema()\ns.col(\"user_id\").is_unique()\ns.col(\"age\").in_range(18, 70)\ns.col(\"plan_type\").is_type(\"object\")\n\n# Validate\nAssertion(df).satisfies(s).no_nulls().validate()\n</code></pre>"},{"location":"data_assertions/#schema-builder","title":"Schema Builder","text":"<p>The <code>schema()</code> builder provides a fluent interface for defining DataFrame validation rules.</p>"},{"location":"data_assertions/#basic-usage","title":"Basic Usage","text":"<pre><code>from ml_assert import schema\n\n# Create a schema\ns = schema()\ns.col(\"user_id\").is_unique()\ns.col(\"age\").in_range(18, 70)\ns.col(\"plan_type\").in_set([\"basic\", \"premium\", \"free\"])\n</code></pre>"},{"location":"data_assertions/#available-validators","title":"Available Validators","text":""},{"location":"data_assertions/#is_unique","title":"<code>is_unique()</code>","text":"<p>Checks if column values are unique.</p>"},{"location":"data_assertions/#in_rangemin_val-max_val","title":"<code>in_range(min_val, max_val)</code>","text":"<p>Checks if column values are within a range.</p> <p>Parameters: - <code>min_val</code>: Minimum allowed value - <code>max_val</code>: Maximum allowed value</p>"},{"location":"data_assertions/#is_typedtype","title":"<code>is_type(dtype)</code>","text":"<p>Checks if column has the specified data type.</p> <p>Parameters: - <code>dtype</code>: Expected data type (e.g., \"int64\", \"float64\", \"object\")</p>"},{"location":"data_assertions/#in_setallowed_values","title":"<code>in_set(allowed_values)</code>","text":"<p>Checks if column values are in a set of allowed values.</p> <p>Parameters: - <code>allowed_values</code>: Set or list of allowed values</p>"},{"location":"data_assertions/#matchespattern","title":"<code>matches(pattern)</code>","text":"<p>Checks if column values match a regex pattern.</p> <p>Parameters: - <code>pattern</code>: Regular expression pattern to match</p>"},{"location":"data_assertions/#is_not_null","title":"<code>is_not_null()</code>","text":"<p>Checks if column has no null values.</p>"},{"location":"data_assertions/#is_sortedascendingtrue","title":"<code>is_sorted(ascending=True)</code>","text":"<p>Checks if column is sorted.</p> <p>Parameters: - <code>ascending</code>: Whether to check ascending order (default: True)</p>"},{"location":"data_assertions/#dataframeassertion","title":"DataFrameAssertion","text":"<p>The main class for DataFrame validation.</p>"},{"location":"data_assertions/#methods","title":"Methods","text":""},{"location":"data_assertions/#satisfiesschema","title":"<code>satisfies(schema)</code>","text":"<p>Validates the DataFrame against a schema definition.</p> <p>Parameters: - <code>schema</code>: A schema object created using the <code>schema()</code> builder</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"data_assertions/#no_nullscolumnsnone","title":"<code>no_nulls(columns=None)</code>","text":"<p>Checks for null values in specified columns.</p> <p>Parameters: - <code>columns</code>: Optional list of column names to check. If None, checks all columns.</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"data_assertions/#validate","title":"<code>validate()</code>","text":"<p>Executes all chained assertions.</p> <p>Raises: - <code>AssertionError</code> if any assertion fails</p>"},{"location":"data_assertions/#error-handling-result-reporting","title":"Error Handling &amp; Result Reporting","text":"<ul> <li>All assertion methods raise <code>AssertionError</code> if a check fails during chaining, unless <code>.validate()</code> is called.</li> <li><code>.validate()</code> returns an <code>AssertionResult</code> object:<ul> <li><code>success</code> (bool): True if all assertions passed.</li> <li><code>message</code> (str): Summary message.</li> <li><code>timestamp</code> (datetime): When the check was run.</li> <li><code>metadata</code> (dict): Details of each assertion (name, args, success, error if any).</li> </ul> </li> </ul>"},{"location":"data_assertions/#examples","title":"Examples","text":""},{"location":"data_assertions/#basic-schema-validation","title":"Basic Schema Validation","text":"<pre><code>from ml_assert import Assertion, schema\n\n# Create a schema\ns = schema()\ns.col(\"id\").is_unique()\ns.col(\"age\").in_range(18, 70)\ns.col(\"email\").matches(r\"^[^@]+@[^@]+\\.[^@]+$\")\n\n# Validate\nAssertion(df).satisfies(s).validate()\n</code></pre>"},{"location":"data_assertions/#complex-schema-with-multiple-rules","title":"Complex Schema with Multiple Rules","text":"<pre><code>from ml_assert import Assertion, schema\n\n# Create a schema\ns = schema()\ns.col(\"user_id\").is_unique().is_not_null()\ns.col(\"age\").in_range(18, 70).is_type(\"int64\")\ns.col(\"plan_type\").in_set([\"basic\", \"premium\", \"free\"])\ns.col(\"subscription_date\").is_sorted(ascending=True)\n\n# Validate\nAssertion(df).satisfies(s).no_nulls([\"user_id\", \"plan_type\"]).validate()\n</code></pre> <p>For more detailed API reference, see Data API.</p>"},{"location":"fairness/","title":"Fairness and Explainability","text":"<p>The <code>ml_assert.fairness</code> module provides tools for assessing model fairness and explainability.</p>"},{"location":"fairness/#fairness-metrics","title":"Fairness Metrics","text":"<p>The <code>FairnessMetrics</code> class provides methods to compute various fairness metrics:</p>"},{"location":"fairness/#demographic-parity","title":"Demographic Parity","text":"<p>Demographic parity measures the difference in positive prediction rates across different groups defined by a sensitive attribute.</p> <pre><code>from ml_assert.fairness.fairness import FairnessMetrics\n\nmetrics = FairnessMetrics(y_true, y_pred, sensitive_attr)\ndp = metrics.demographic_parity()\n</code></pre>"},{"location":"fairness/#equal-opportunity","title":"Equal Opportunity","text":"<p>Equal opportunity measures the difference in true positive rates across different groups.</p> <pre><code>eo = metrics.equal_opportunity()\n</code></pre>"},{"location":"fairness/#model-explainability","title":"Model Explainability","text":"<p>The <code>ModelExplainer</code> class uses SHAP (SHapley Additive exPlanations) to explain model predictions:</p> <pre><code>from ml_assert.fairness.explainability import ModelExplainer\n\nexplainer = ModelExplainer(model, feature_names=X.columns)\n</code></pre>"},{"location":"fairness/#basic-usage","title":"Basic Usage","text":"<p>Generate SHAP values for your model:</p> <pre><code>shap_values = explainer.explain(X)\n</code></pre>"},{"location":"fairness/#visualization","title":"Visualization","text":"<p>Generate various plots to understand model behavior:</p> <pre><code># Summary plot of feature importance\nexplainer.plot_summary(X, output_path=\"summary_plot.png\")\n\n# Dependence plot for a specific feature\nexplainer.plot_dependence(X, feature=\"age\", output_path=\"dependence_age.png\")\n\n# Dependence plot with interaction\nexplainer.plot_dependence(\n    X,\n    feature=\"income\",\n    interaction_index=\"education\",\n    output_path=\"dependence_income_education.png\"\n)\n</code></pre>"},{"location":"fairness/#feature-analysis","title":"Feature Analysis","text":"<p>Analyze feature importance and interactions:</p> <pre><code># Get feature importance scores\nimportance_df = explainer.get_feature_importance(X)\n\n# Analyze feature interactions\ninteractions = explainer.analyze_interactions(X, top_n=5)\n</code></pre>"},{"location":"fairness/#comprehensive-reports","title":"Comprehensive Reports","text":"<p>Generate a complete explanation report:</p> <pre><code>explainer.save_explanation_report(\n    X,\n    output_dir=\"explanation_report\",\n    include_plots=True\n)\n</code></pre> <p>This will generate: - Raw SHAP values (<code>shap_values.npy</code>) - Feature importance CSV (<code>feature_importance.csv</code>) - Feature interactions CSV (<code>feature_interactions.csv</code>) - Summary plot (<code>summary_plot.png</code>) - Dependence plots for top features</p>"},{"location":"fairness/#using-in-yaml-config","title":"Using in YAML Config","text":"<p>You can include fairness and explainability checks in your YAML configuration:</p> <pre><code>steps:\n  # Fairness checks\n  - type: fairness\n    y_true: \"data/y_true.csv\"\n    y_pred: \"data/y_pred.csv\"\n    sensitive_attr: \"data/sensitive_attr.csv\"\n    demographic_parity: 0.1  # Maximum allowed difference\n    equal_opportunity: 0.1   # Maximum allowed difference\n\n  # Model explainability with comprehensive report\n  - type: explainability\n    model: \"models/trained_model.pkl\"\n    features: \"data/features.csv\"\n    output_dir: \"results/explanation_report\"\n    include_plots: true\n    plots:\n      summary:\n        output: \"results/explanation_report/summary_plot.png\"\n      dependence:\n        - feature: \"age\"\n          output: \"results/explanation_report/dependence_age.png\"\n        - feature: \"income\"\n          interaction_index: \"education\"\n          output: \"results/explanation_report/dependence_income_education.png\"\n</code></pre>"},{"location":"fairness/#fairness-step","title":"Fairness Step","text":"<p>The fairness step checks if your model's predictions satisfy fairness criteria:</p> <ul> <li><code>y_true</code>: Path to ground truth labels</li> <li><code>y_pred</code>: Path to model predictions</li> <li><code>sensitive_attr</code>: Path to sensitive attribute values</li> <li><code>demographic_parity</code>: Maximum allowed difference in positive prediction rates</li> <li><code>equal_opportunity</code>: Maximum allowed difference in true positive rates</li> </ul>"},{"location":"fairness/#explainability-step","title":"Explainability Step","text":"<p>The explainability step provides comprehensive model explanations:</p> <ul> <li><code>model</code>: Path to the trained model</li> <li><code>features</code>: Path to feature data</li> <li><code>output_dir</code>: Directory to save the explanation report (optional)</li> <li><code>include_plots</code>: Whether to generate visualization plots (default: true)</li> <li><code>plots</code>: Configuration for specific plots:</li> <li><code>summary</code>: Generate a summary plot of feature importance</li> <li><code>dependence</code>: Generate dependence plots for specific features<ul> <li><code>feature</code>: Name of the feature to plot</li> <li><code>interaction_index</code>: Optional feature to show interaction with</li> <li><code>output</code>: Path to save the plot</li> </ul> </li> </ul>"},{"location":"integrations/","title":"Integrations","text":"<p>ml-assert provides integrations for alerting and monitoring assertion results in production ML workflows. The main integrations are:</p> <ul> <li>SlackAlerter: Send alerts to a Slack channel via webhook.</li> <li>PrometheusExporter: Expose assertion results as Prometheus metrics.</li> <li>MLflowLogger: Log assertion results to MLflow experiments.</li> </ul>"},{"location":"integrations/#api-reference","title":"API Reference","text":"<p>(See below for each integration's API and usage.)</p>"},{"location":"integrations/#error-handling-result-reporting","title":"Error Handling &amp; Result Reporting","text":"<ul> <li>All integrations accept an <code>AssertionResult</code> object for reporting.</li> <li>Integrations use the <code>success</code>, <code>message</code>, and <code>metadata</code> fields for alerting, logging, and monitoring.</li> <li>If an integration fails (e.g., network error), the CLI logs a warning but does not stop the workflow.</li> <li>Use the <code>metadata</code> field to enrich alerts and logs with additional context.</li> </ul>"},{"location":"integrations/#integration-workflow-diagram","title":"Integration Workflow Diagram","text":"<pre><code>graph TD\n    A[AssertionResult produced] --&gt; B[SlackAlerter.send_alert]\n    A --&gt; C[PrometheusExporter.record_assertion]\n    A --&gt; D[MLflowLogger.log_assertion_result_mlassert]\n    B &amp; C &amp; D --&gt; E[User/monitoring system receives alert or metrics]\n</code></pre>"},{"location":"integrations/#advanced-usage-combining-integrations","title":"Advanced Usage: Combining Integrations","text":"<p>You can combine multiple integrations in a workflow for robust monitoring and alerting.</p> <p>Example:</p> <pre><code>from ml_assert.integrations.slack import SlackAlerter\nfrom ml_assert.integrations.prometheus import PrometheusExporter\nfrom ml_assert.integrations.mlflow import MLflowLogger\nfrom ml_assert.core.base import AssertionResult\nfrom datetime import datetime\n\n# Set up integrations\nslack = SlackAlerter(webhook_url=\"https://hooks.slack.com/services/your/webhook/url\")\nprom = PrometheusExporter(port=8000)\nprom.start()\nmlflow_logger = MLflowLogger(experiment_name=\"my-experiment\")\nmlflow_logger.start_run()\n\n# Run an assertion and report\nresult = AssertionResult(success=False, message=\"Test failed\", timestamp=datetime.now(), metadata={\"step\": \"validation\"})\nslack.send_alert(result, title=\"ML Assertion Failure\")\nprom.record_assertion(result)\nmlflow_logger.log_assertion_result_mlassert(result, step_name=\"validation_step\")\nmlflow_logger.end_run()\n</code></pre>"},{"location":"integrations/#slackalerter","title":"SlackAlerter","text":"<p>Send alerts to a Slack channel when assertions fail.</p> <p>Class: <code>ml_assert.integrations.slack.SlackAlerter</code></p> <pre><code>from ml_assert.integrations.slack import SlackAlerter\nfrom ml_assert.core.base import AssertionResult\nfrom datetime import datetime\n\nalerter = SlackAlerter(webhook_url=\"https://hooks.slack.com/services/your/webhook/url\")\nresult = AssertionResult(success=False, message=\"Test failure\", timestamp=datetime.now(), metadata={})\nalerter.send_alert(result, title=\"ML Assertion Failure\")\n</code></pre> <ul> <li><code>webhook_url</code> (str): Your Slack webhook URL.</li> <li><code>send_alert(result, title=None)</code>: Send an alert for an <code>AssertionResult</code>.</li> </ul>"},{"location":"integrations/#prometheusexporter","title":"PrometheusExporter","text":"<p>Expose assertion results as Prometheus metrics for monitoring.</p> <p>Class: <code>ml_assert.integrations.prometheus.PrometheusExporter</code></p> <pre><code>from ml_assert.integrations.prometheus import PrometheusExporter\nfrom ml_assert.core.base import AssertionResult\nfrom datetime import datetime\n\nexporter = PrometheusExporter(port=8000)\nexporter.start()\nresult = AssertionResult(success=True, message=\"Test passed\", timestamp=datetime.now(), metadata={})\nexporter.record_assertion(result)\n</code></pre> <ul> <li><code>port</code> (int): Port to serve metrics (default: 8000).</li> <li><code>start()</code>: Start the HTTP server for Prometheus scraping.</li> <li><code>record_assertion(result)</code>: Record an <code>AssertionResult</code> as metrics.</li> </ul>"},{"location":"integrations/#mlflowlogger","title":"MLflowLogger","text":"<p>Log assertion results to MLflow for experiment tracking and auditing.</p> <p>Class: <code>ml_assert.integrations.mlflow.MLflowLogger</code></p> <pre><code>from ml_assert.integrations.mlflow import MLflowLogger\nfrom ml_assert.core.base import AssertionResult\nfrom datetime import datetime\n\nlogger = MLflowLogger(experiment_name=\"my-experiment\")\nlogger.start_run()\nresult = AssertionResult(success=True, message=\"Test passed\", timestamp=datetime.now(), metadata={\"step\": \"validation\"})\nlogger.log_assertion_result_mlassert(result, step_name=\"validation_step\")\nlogger.end_run()\n</code></pre> <ul> <li><code>experiment_name</code> (str): Name of the MLflow experiment.</li> <li><code>run_name</code> (str, optional): Name for the MLflow run.</li> <li><code>tracking_uri</code> (str, optional): MLflow tracking server URI.</li> <li><code>start_run()</code>: Start a new MLflow run.</li> <li><code>end_run(status=\"FINISHED\")</code>: End the current run.</li> <li><code>log_assertion_result_mlassert(result, step_name=None)</code>: Log an <code>AssertionResult</code> to MLflow.</li> </ul>"},{"location":"integrations/#example-yaml-configuration","title":"Example: YAML Configuration","text":"<p>You can enable integrations in your YAML config for the CLI runner:</p> <pre><code>slack_webhook: \"https://hooks.slack.com/services/your/webhook/url\"\nprometheus_port: 8000\nsteps:\n  - type: model_performance\n    y_true: \"data/y_true.csv\"\n    y_pred: \"data/y_pred.csv\"\n    assertions:\n      accuracy: 0.8\n      f1: 0.7\n</code></pre> <ul> <li>Slack alerts will be sent for failed assertions.</li> <li>Prometheus metrics will be exposed on the specified port.</li> </ul>"},{"location":"model_assertions/","title":"Model Performance Assertions","text":"<p>The <code>ml_assert</code> module provides a fluent interface for validating model performance metrics.</p>"},{"location":"model_assertions/#quick-start","title":"Quick Start","text":"<pre><code>from ml_assert import assert_model\n\n# Assert model performance\nassert_model(y_true, y_pred, y_scores) \\\n    .accuracy(min_score=0.8) \\\n    .precision(min_score=0.8) \\\n    .recall(min_score=0.8) \\\n    .f1(min_score=0.8) \\\n    .roc_auc(min_score=0.9) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#available-metrics","title":"Available Metrics","text":""},{"location":"model_assertions/#accuracy","title":"Accuracy","text":"<p>Measures the proportion of correct predictions.</p> <pre><code>assert_model(y_true, y_pred, y_scores) \\\n    .accuracy(min_score=0.8) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#precision","title":"Precision","text":"<p>Measures the ability of the classifier not to label as positive a sample that is negative.</p> <pre><code>assert_model(y_true, y_pred, y_scores) \\\n    .precision(min_score=0.8) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#recall","title":"Recall","text":"<p>Measures the ability of the classifier to find all the positive samples.</p> <pre><code>assert_model(y_true, y_pred, y_scores) \\\n    .recall(min_score=0.8) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#f1-score","title":"F1 Score","text":"<p>The harmonic mean of precision and recall.</p> <pre><code>assert_model(y_true, y_pred, y_scores) \\\n    .f1(min_score=0.8) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#roc-auc","title":"ROC AUC","text":"<p>Area Under the Receiver Operating Characteristic Curve, measuring the ability to distinguish between classes.</p> <pre><code>assert_model(y_true, y_pred, y_scores) \\\n    .roc_auc(min_score=0.9) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#error-handling-result-reporting","title":"Error Handling &amp; Result Reporting","text":"<ul> <li>All assertion methods raise <code>AssertionError</code> if a check fails during chaining, unless <code>.validate()</code> is called.</li> <li><code>.validate()</code> returns an <code>AssertionResult</code> object:<ul> <li><code>success</code> (bool): True if all assertions passed.</li> <li><code>message</code> (str): Summary message.</li> <li><code>timestamp</code> (datetime): When the check was run.</li> <li><code>metadata</code> (dict): Details of each assertion (name, args, success, error if any).</li> </ul> </li> </ul>"},{"location":"model_assertions/#examples","title":"Examples","text":""},{"location":"model_assertions/#basic-performance-check","title":"Basic Performance Check","text":"<pre><code>from ml_assert import assert_model\n\n# Check basic performance metrics\nassert_model(y_true, y_pred, y_scores) \\\n    .accuracy(min_score=0.8) \\\n    .precision(min_score=0.8) \\\n    .recall(min_score=0.8) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#comprehensive-performance-check","title":"Comprehensive Performance Check","text":"<pre><code>from ml_assert import assert_model\n\n# Check all performance metrics\nassert_model(y_true, y_pred, y_scores) \\\n    .accuracy(min_score=0.8) \\\n    .precision(min_score=0.8) \\\n    .recall(min_score=0.8) \\\n    .f1(min_score=0.8) \\\n    .roc_auc(min_score=0.9) \\\n    .validate()\n</code></pre>"},{"location":"model_assertions/#custom-thresholds","title":"Custom Thresholds","text":"<pre><code>from ml_assert import assert_model\n\n# Use different thresholds for different metrics\nassert_model(y_true, y_pred, y_scores) \\\n    .accuracy(min_score=0.75) \\\n    .precision(min_score=0.85) \\\n    .recall(min_score=0.70) \\\n    .f1(min_score=0.80) \\\n    .roc_auc(min_score=0.90) \\\n    .validate()\n</code></pre> <p>For more detailed API reference, see Model API.</p>"},{"location":"plugins/","title":"Plugins","text":"<p>The <code>ml-assert</code> library can be extended with plugins to add custom checks. Plugins are discovered automatically if they are installed in the same environment and registered via entry points.</p>"},{"location":"plugins/#api-reference-plugin","title":"API Reference: Plugin","text":""},{"location":"plugins/#class-plugin","title":"<code>class Plugin</code>","text":"<p>Abstract base class for all plugins.</p> <p>Method: - <code>run(self, config: dict) -&gt; AssertionResult</code>: Execute the plugin's logic. Must return an <code>AssertionResult</code>.</p> <p>Example:</p> <pre><code>from ml_assert.plugins.base import Plugin\nfrom ml_assert.core.base import AssertionResult\nfrom datetime import datetime\n\nclass MyPlugin(Plugin):\n    def run(self, config: dict) -&gt; AssertionResult:\n        # ... your logic ...\n        if some_condition:\n            return AssertionResult(\n                success=True,\n                message=\"Check passed!\",\n                timestamp=datetime.now(),\n                metadata={\"info\": \"details\"}\n            )\n        else:\n            return AssertionResult(\n                success=False,\n                message=\"Check failed!\",\n                timestamp=datetime.now(),\n                metadata={\"error\": \"details\"}\n            )\n</code></pre>"},{"location":"plugins/#error-handling-result-reporting","title":"Error Handling &amp; Result Reporting","text":"<ul> <li>Plugins must return an <code>AssertionResult</code> from <code>run()</code>.</li> <li>The CLI and integrations use the <code>success</code>, <code>message</code>, and <code>metadata</code> fields for reporting and alerting.</li> <li>If a plugin raises an exception, it is caught by the CLI and reported as a failed step.</li> <li>Use the <code>metadata</code> field to provide detailed results or debugging info.</li> </ul>"},{"location":"plugins/#plugin-workflow-diagram","title":"Plugin Workflow Diagram","text":"<pre><code>graph TD\n    A[CLI loads plugins] --&gt; B[Plugin.run(config)]\n    B --&gt; C{Check passes?}\n    C -- Yes --&gt; D[Return AssertionResult(success=True)]\n    C -- No --&gt; E[Return AssertionResult(success=False, metadata with errors)]\n    D &amp; E --&gt; F[CLI/integrations handle result]\n</code></pre>"},{"location":"plugins/#advanced-usage-custom-plugin-example","title":"Advanced Usage: Custom Plugin Example","text":"<p>Suppose you want to check that a DataFrame has at least N rows:</p> <pre><code>from ml_assert.plugins.base import Plugin\nfrom ml_assert.core.base import AssertionResult\nfrom datetime import datetime\nimport pandas as pd\n\nclass MinRowsPlugin(Plugin):\n    def run(self, config: dict) -&gt; AssertionResult:\n        df = pd.read_csv(config[\"file\"])\n        min_rows = config.get(\"min_rows\", 10)\n        if len(df) &gt;= min_rows:\n            return AssertionResult(\n                success=True,\n                message=f\"DataFrame has {len(df)} rows (min required: {min_rows})\",\n                timestamp=datetime.now(),\n                metadata={\"row_count\": len(df)}\n            )\n        else:\n            return AssertionResult(\n                success=False,\n                message=f\"DataFrame has only {len(df)} rows (min required: {min_rows})\",\n                timestamp=datetime.now(),\n                metadata={\"row_count\": len(df)}\n            )\n</code></pre> <p>Register your plugin in your <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"ml_assert.plugins\"]\nmin_rows = \"my_package.plugins:MinRowsPlugin\"\n</code></pre> <p>Use in YAML config:</p> <pre><code>steps:\n  - type: min_rows\n    file: data/my_data.csv\n    min_rows: 100\n</code></pre>"},{"location":"plugins/#built-in-plugins","title":"Built-in Plugins","text":""},{"location":"plugins/#fileexistsplugin","title":"<code>FileExistsPlugin</code>","text":"<p>Asserts that a file exists at a given path.</p> <ul> <li>Type Name: <code>file_exists</code></li> <li>Configuration:<ul> <li><code>path</code> (<code>str</code>): The path to the file to check.</li> </ul> </li> <li>Example <code>config.yaml</code>:     ```yaml     steps:<ul> <li>type: file_exists     path: path/to/my/file.csv ```</li> </ul> </li> </ul>"},{"location":"plugins/#dvcartifactcheckplugin","title":"<code>DVCArtifactCheckPlugin</code>","text":"<p>Asserts that a DVC-tracked artifact is in sync with its <code>.dvc</code> file, meaning it has not been modified since <code>dvc add</code>.</p> <ul> <li>Type Name: <code>dvc_check</code></li> <li>Installation: Requires the <code>dvc</code> extra. Install with <code>pip install ml-assert[dvc]</code>.</li> <li>Configuration:<ul> <li><code>path</code> (<code>str</code>): The path to the DVC-tracked artifact.</li> </ul> </li> <li>Example <code>config.yaml</code>:     ```yaml     steps:<ul> <li>type: dvc_check     path: data/raw/training_data.csv ```</li> </ul> </li> </ul>"},{"location":"plugins/#creating-your-own-plugin","title":"Creating Your Own Plugin","text":"<p>To create a custom plugin:</p> <ol> <li>Inherit from <code>ml_assert.plugins.base.Plugin</code>.</li> <li>Implement <code>run(self, config: dict) -&gt; AssertionResult</code>.</li> <li> <p>Register your plugin as an entry point in your package's <code>pyproject.toml</code> under the <code>ml_assert.plugins</code> group.</p> <p><code>toml [project.entry-points.\"ml_assert.plugins\"] my_plugin_name = \"my_package.plugins:MyCustomPlugin\"</code></p> </li> </ol>"},{"location":"statistical_assertions/","title":"Statistical Assertions","text":"<p>The <code>ml_assert.stats</code> module provides functions for statistical comparison and drift detection between datasets.</p>"},{"location":"statistical_assertions/#quick-start","title":"Quick Start","text":"<pre><code>from ml_assert.stats.drift import assert_no_drift\n\n# Check for drift between reference and current datasets\nassert_no_drift(df_ref, df_cur, alpha=0.05)\n</code></pre>"},{"location":"statistical_assertions/#drift-detection","title":"Drift Detection","text":""},{"location":"statistical_assertions/#high-level-drift-detection","title":"High-Level Drift Detection","text":"<p>The <code>assert_no_drift</code> function automatically detects drift in all columns of a DataFrame.</p> <pre><code>from ml_assert.stats.drift import assert_no_drift\n\n# Check for drift between training and test sets\nassert_no_drift(df_train, df_test, alpha=0.05)\n</code></pre> <p>Parameters: - <code>df_ref</code>: Reference DataFrame (e.g., training data) - <code>df_cur</code>: Current DataFrame (e.g., inference data) - <code>alpha</code>: Significance level for statistical tests (default: 0.05)</p>"},{"location":"statistical_assertions/#low-level-statistical-tests","title":"Low-Level Statistical Tests","text":""},{"location":"statistical_assertions/#kolmogorov-smirnov-test","title":"Kolmogorov-Smirnov Test","text":"<p>For comparing continuous distributions.</p> <pre><code>from ml_assert.stats.drift import ks_test\n\n# Compare two numeric samples\nstatistic, p_value = ks_test(sample1, sample2, alpha=0.05)\n</code></pre>"},{"location":"statistical_assertions/#chi-squared-test","title":"Chi-Squared Test","text":"<p>For comparing categorical distributions.</p> <pre><code>from ml_assert.stats.drift import chi2_test\n\n# Compare two categorical samples\nstatistic, p_value = chi2_test(sample1, sample2, alpha=0.05)\n</code></pre>"},{"location":"statistical_assertions/#wasserstein-distance","title":"Wasserstein Distance","text":"<p>For measuring the distance between distributions.</p> <pre><code>from ml_assert.stats.drift import wasserstein_distance\n\n# Calculate Wasserstein distance\ndistance = wasserstein_distance(sample1, sample2)\n</code></pre>"},{"location":"statistical_assertions/#distribution-assertions","title":"Distribution Assertions","text":""},{"location":"statistical_assertions/#distribution-testing","title":"Distribution Testing","text":"<p>Assert that a dataset follows a specific distribution.</p> <pre><code>from ml_assert.stats.distribution import assert_distribution\n\n# Assert normal distribution\nassert_distribution(data, distribution=\"normal\", alpha=0.05)\n</code></pre> <p>Supported Distributions: - \"normal\" - \"uniform\" - \"exponential\" - \"poisson\"</p>"},{"location":"statistical_assertions/#examples","title":"Examples","text":""},{"location":"statistical_assertions/#basic-drift-detection","title":"Basic Drift Detection","text":"<pre><code>from ml_assert.stats.drift import assert_no_drift\n\n# Check for drift in all columns\nassert_no_drift(df_train, df_test, alpha=0.05)\n</code></pre>"},{"location":"statistical_assertions/#custom-statistical-tests","title":"Custom Statistical Tests","text":"<pre><code>from ml_assert.stats.drift import ks_test, chi2_test\n\n# Compare numeric columns\nstatistic, p_value = ks_test(df_train[\"age\"], df_test[\"age\"], alpha=0.05)\n\n# Compare categorical columns\nstatistic, p_value = chi2_test(df_train[\"category\"], df_test[\"category\"], alpha=0.05)\n</code></pre>"},{"location":"statistical_assertions/#distribution-testing_1","title":"Distribution Testing","text":"<pre><code>from ml_assert.stats.distribution import assert_distribution\n\n# Test for normal distribution\nassert_distribution(data, distribution=\"normal\", alpha=0.05)\n\n# Test for uniform distribution\nassert_distribution(data, distribution=\"uniform\", alpha=0.05)\n</code></pre> <p>For more detailed API reference, see Stats API.</p>"},{"location":"api/core/","title":"Core API Reference","text":""},{"location":"api/core/#dataframeassertion","title":"DataFrameAssertion","text":"<p>The main assertion class for DataFrame validation.</p> <pre><code>from ml_assert import Assertion\n\n# Create an assertion instance\nassertion = Assertion(df)\n\n# Chain assertions\nassertion.satisfies(schema).no_nulls().validate()\n</code></pre>"},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#satisfiesschema","title":"<code>satisfies(schema)</code>","text":"<p>Validates the DataFrame against a schema definition.</p> <p>Parameters: - <code>schema</code>: A schema object created using the <code>schema()</code> builder</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"api/core/#no_nullscolumnsnone","title":"<code>no_nulls(columns=None)</code>","text":"<p>Checks for null values in specified columns.</p> <p>Parameters: - <code>columns</code>: Optional list of column names to check. If None, checks all columns.</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"api/core/#validate","title":"<code>validate()</code>","text":"<p>Executes all chained assertions.</p> <p>Raises: - <code>AssertionError</code> if any assertion fails</p>"},{"location":"api/core/#schema","title":"schema","text":"<p>A builder for creating DataFrame validation schemas.</p> <pre><code>from ml_assert import schema\n\n# Create a schema\ns = schema()\ns.col(\"user_id\").is_unique()\ns.col(\"age\").in_range(18, 70)\n</code></pre>"},{"location":"api/core/#methods_1","title":"Methods","text":""},{"location":"api/core/#colcolumn_name","title":"<code>col(column_name)</code>","text":"<p>Starts a column validation chain.</p> <p>Parameters: - <code>column_name</code>: Name of the column to validate</p> <p>Returns: - A column validator object</p>"},{"location":"api/core/#column-validator-methods","title":"Column Validator Methods","text":""},{"location":"api/core/#is_unique","title":"<code>is_unique()</code>","text":"<p>Checks if column values are unique.</p>"},{"location":"api/core/#in_rangemin_val-max_val","title":"<code>in_range(min_val, max_val)</code>","text":"<p>Checks if column values are within a range.</p> <p>Parameters: - <code>min_val</code>: Minimum allowed value - <code>max_val</code>: Maximum allowed value</p>"},{"location":"api/core/#is_typedtype","title":"<code>is_type(dtype)</code>","text":"<p>Checks if column has the specified data type.</p> <p>Parameters: - <code>dtype</code>: Expected data type (e.g., \"int64\", \"float64\", \"object\")</p>"},{"location":"api/fairness/","title":"Fairness API Reference","text":""},{"location":"api/fairness/#fairness-metrics","title":"Fairness Metrics","text":""},{"location":"api/fairness/#assert_fairness","title":"assert_fairness","text":"<p>High-level function to assert fairness metrics for a model.</p> <pre><code>from ml_assert.fairness import assert_fairness\n\n# Assert fairness metrics\nassert_fairness(\n    y_true=y_true,\n    y_pred=y_pred,\n    sensitive_features=sensitive_features,\n    metrics=[\"demographic_parity\", \"equal_opportunity\"],\n    threshold=0.1\n)\n</code></pre>"},{"location":"api/fairness/#parameters","title":"Parameters","text":"<ul> <li><code>y_true</code>: True labels</li> <li><code>y_pred</code>: Predicted labels</li> <li><code>sensitive_features</code>: Array of sensitive feature values</li> <li><code>metrics</code>: List of fairness metrics to check</li> <li><code>threshold</code>: Maximum allowed difference between groups (default: 0.1)</li> </ul>"},{"location":"api/fairness/#supported-metrics","title":"Supported Metrics","text":"<ul> <li><code>demographic_parity</code>: Equal positive prediction rates across groups</li> <li><code>equal_opportunity</code>: Equal true positive rates across groups</li> <li><code>equalized_odds</code>: Equal true positive and false positive rates</li> <li><code>treatment_equality</code>: Equal ratio of false negatives to false positives</li> </ul>"},{"location":"api/fairness/#explainability","title":"Explainability","text":""},{"location":"api/fairness/#assert_feature_importance","title":"assert_feature_importance","text":"<p>Assert minimum feature importance scores.</p> <pre><code>from ml_assert.fairness import assert_feature_importance\n\n# Assert feature importance\nassert_feature_importance(\n    model=model,\n    X=X,\n    min_importance=0.1,\n    features=[\"feature1\", \"feature2\"]\n)\n</code></pre>"},{"location":"api/fairness/#parameters_1","title":"Parameters","text":"<ul> <li><code>model</code>: Trained model with feature_importances_ attribute</li> <li><code>X</code>: Feature matrix</li> <li><code>min_importance</code>: Minimum importance score (0.0 to 1.0)</li> <li><code>features</code>: List of features to check (optional)</li> </ul>"},{"location":"api/fairness/#assert_shap_values","title":"assert_shap_values","text":"<p>Assert SHAP values for feature importance.</p> <pre><code>from ml_assert.fairness import assert_shap_values\n\n# Assert SHAP values\nassert_shap_values(\n    model=model,\n    X=X,\n    min_importance=0.1,\n    features=[\"feature1\", \"feature2\"]\n)\n</code></pre>"},{"location":"api/fairness/#parameters_2","title":"Parameters","text":"<ul> <li><code>model</code>: Trained model</li> <li><code>X</code>: Feature matrix</li> <li><code>min_importance</code>: Minimum absolute SHAP value</li> <li><code>features</code>: List of features to check (optional)</li> </ul>"},{"location":"api/integrations/","title":"Integrations API Reference","text":""},{"location":"api/integrations/#mlflow-integration","title":"MLflow Integration","text":""},{"location":"api/integrations/#assert_mlflow_model","title":"assert_mlflow_model","text":"<p>Assert MLflow model properties and metrics.</p> <pre><code>from ml_assert.integrations.mlflow import assert_mlflow_model\n\n# Assert MLflow model\nassert_mlflow_model(\n    model_uri=\"runs:/run_id/model\",\n    metrics={\n        \"accuracy\": 0.8,\n        \"precision\": 0.8,\n        \"recall\": 0.8\n    }\n)\n</code></pre>"},{"location":"api/integrations/#parameters","title":"Parameters","text":"<ul> <li><code>model_uri</code>: MLflow model URI</li> <li><code>metrics</code>: Dictionary of metric names and minimum values</li> <li><code>params</code>: Dictionary of parameter names and expected values (optional)</li> </ul>"},{"location":"api/integrations/#prometheus-integration","title":"Prometheus Integration","text":""},{"location":"api/integrations/#assert_prometheus_metrics","title":"assert_prometheus_metrics","text":"<p>Assert Prometheus metrics for model monitoring.</p> <pre><code>from ml_assert.integrations.prometheus import assert_prometheus_metrics\n\n# Assert Prometheus metrics\nassert_prometheus_metrics(\n    metrics={\n        \"model_accuracy\": 0.8,\n        \"prediction_latency\": 100\n    },\n    labels={\"model\": \"my_model\"}\n)\n</code></pre>"},{"location":"api/integrations/#parameters_1","title":"Parameters","text":"<ul> <li><code>metrics</code>: Dictionary of metric names and thresholds</li> <li><code>labels</code>: Dictionary of label names and values</li> <li><code>timeout</code>: Maximum time to wait for metrics (default: 30 seconds)</li> </ul>"},{"location":"api/integrations/#slack-integration","title":"Slack Integration","text":""},{"location":"api/integrations/#assert_slack_notification","title":"assert_slack_notification","text":"<p>Assert Slack notification delivery.</p> <pre><code>from ml_assert.integrations.slack import assert_slack_notification\n\n# Assert Slack notification\nassert_slack_notification(\n    channel=\"#ml-monitoring\",\n    message=\"Model drift detected\",\n    timeout=30\n)\n</code></pre>"},{"location":"api/integrations/#parameters_2","title":"Parameters","text":"<ul> <li><code>channel</code>: Slack channel name</li> <li><code>message</code>: Expected message content</li> <li><code>timeout</code>: Maximum time to wait for notification (default: 30 seconds)</li> </ul>"},{"location":"api/integrations/#dvc-integration","title":"DVC Integration","text":""},{"location":"api/integrations/#assert_dvc_artifact","title":"assert_dvc_artifact","text":"<p>Assert DVC artifact properties.</p> <pre><code>from ml_assert.integrations.dvc import assert_dvc_artifact\n\n# Assert DVC artifact\nassert_dvc_artifact(\n    path=\"models/model.pkl\",\n    exists=True,\n    size_mb=10\n)\n</code></pre>"},{"location":"api/integrations/#parameters_3","title":"Parameters","text":"<ul> <li><code>path</code>: Path to DVC artifact</li> <li><code>exists</code>: Whether artifact should exist</li> <li><code>size_mb</code>: Expected size in megabytes (optional)</li> <li><code>md5</code>: Expected MD5 hash (optional)</li> </ul>"},{"location":"api/model/","title":"Model API Reference","text":""},{"location":"api/model/#assert_model","title":"assert_model","text":"<p>A builder for model performance assertions.</p> <pre><code>from ml_assert import assert_model\n\n# Create model assertions\nassert_model(y_true, y_pred, y_scores) \\\n    .accuracy(min_score=0.80) \\\n    .precision(min_score=0.80) \\\n    .recall(min_score=0.80) \\\n    .f1(min_score=0.80) \\\n    .roc_auc(min_score=0.90) \\\n    .validate()\n</code></pre>"},{"location":"api/model/#methods","title":"Methods","text":""},{"location":"api/model/#accuracymin_score","title":"<code>accuracy(min_score)</code>","text":"<p>Asserts minimum accuracy score.</p> <p>Parameters: - <code>min_score</code>: Minimum acceptable accuracy score (0.0 to 1.0)</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"api/model/#precisionmin_score","title":"<code>precision(min_score)</code>","text":"<p>Asserts minimum precision score.</p> <p>Parameters: - <code>min_score</code>: Minimum acceptable precision score (0.0 to 1.0)</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"api/model/#recallmin_score","title":"<code>recall(min_score)</code>","text":"<p>Asserts minimum recall score.</p> <p>Parameters: - <code>min_score</code>: Minimum acceptable recall score (0.0 to 1.0)</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"api/model/#f1min_score","title":"<code>f1(min_score)</code>","text":"<p>Asserts minimum F1 score.</p> <p>Parameters: - <code>min_score</code>: Minimum acceptable F1 score (0.0 to 1.0)</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"api/model/#roc_aucmin_score","title":"<code>roc_auc(min_score)</code>","text":"<p>Asserts minimum ROC AUC score.</p> <p>Parameters: - <code>min_score</code>: Minimum acceptable ROC AUC score (0.0 to 1.0)</p> <p>Returns: - <code>self</code> for method chaining</p>"},{"location":"api/model/#validate","title":"<code>validate()</code>","text":"<p>Executes all chained assertions.</p> <p>Raises: - <code>AssertionError</code> if any assertion fails</p>"},{"location":"api/plugins/","title":"Plugins API Reference","text":""},{"location":"api/plugins/#creating-custom-plugins","title":"Creating Custom Plugins","text":""},{"location":"api/plugins/#plugin-base-class","title":"Plugin Base Class","text":"<p>All plugins must inherit from the base plugin class:</p> <pre><code>from ml_assert.plugins.base import Plugin\n\nclass MyCustomPlugin(Plugin):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def validate(self):\n        # Implement validation logic\n        pass\n</code></pre>"},{"location":"api/plugins/#required-methods","title":"Required Methods","text":""},{"location":"api/plugins/#validate","title":"<code>validate()</code>","text":"<p>Must be implemented by all plugins. Should raise <code>AssertionError</code> if validation fails.</p>"},{"location":"api/plugins/#optional-methods","title":"Optional Methods","text":""},{"location":"api/plugins/#setup","title":"<code>setup()</code>","text":"<p>Called before validation. Use for initialization.</p>"},{"location":"api/plugins/#teardown","title":"<code>teardown()</code>","text":"<p>Called after validation. Use for cleanup.</p>"},{"location":"api/plugins/#built-in-plugins","title":"Built-in Plugins","text":""},{"location":"api/plugins/#fileexistsplugin","title":"FileExistsPlugin","text":"<p>Checks if files exist.</p> <pre><code>from ml_assert.plugins.file_exists import FileExistsPlugin\n\n# Check if file exists\nplugin = FileExistsPlugin(path=\"models/model.pkl\")\nplugin.validate()\n</code></pre>"},{"location":"api/plugins/#parameters","title":"Parameters","text":"<ul> <li><code>path</code>: Path to file</li> <li><code>exists</code>: Whether file should exist (default: True)</li> </ul>"},{"location":"api/plugins/#dvcartifactcheckplugin","title":"DVCArtifactCheckPlugin","text":"<p>Checks DVC artifacts.</p> <pre><code>from ml_assert.plugins.dvc_check import DVCArtifactCheckPlugin\n\n# Check DVC artifact\nplugin = DVCArtifactCheckPlugin(\n    path=\"models/model.pkl\",\n    exists=True,\n    size_mb=10\n)\nplugin.validate()\n</code></pre>"},{"location":"api/plugins/#parameters_1","title":"Parameters","text":"<ul> <li><code>path</code>: Path to DVC artifact</li> <li><code>exists</code>: Whether artifact should exist</li> <li><code>size_mb</code>: Expected size in megabytes (optional)</li> <li><code>md5</code>: Expected MD5 hash (optional)</li> </ul>"},{"location":"api/plugins/#plugin-registration","title":"Plugin Registration","text":""},{"location":"api/plugins/#entry-point","title":"Entry Point","text":"<p>Register your plugin in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"ml_assert.plugins\"]\nmy_plugin = \"my_package.plugins:MyCustomPlugin\"\n</code></pre>"},{"location":"api/plugins/#cli-usage","title":"CLI Usage","text":"<p>Use registered plugins in your YAML configuration:</p> <pre><code>steps:\n  - type: my_plugin\n    path: \"models/model.pkl\"\n    exists: true\n</code></pre>"},{"location":"api/stats/","title":"Statistical API Reference","text":""},{"location":"api/stats/#assert_no_drift","title":"assert_no_drift","text":"<p>High-level function to detect distributional drift between datasets.</p> <pre><code>from ml_assert.stats.drift import assert_no_drift\n\n# Check for drift between reference and current datasets\nassert_no_drift(df_ref, df_cur, alpha=0.05)\n</code></pre>"},{"location":"api/stats/#parameters","title":"Parameters","text":"<ul> <li><code>df_ref</code>: Reference DataFrame (e.g., training data)</li> <li><code>df_cur</code>: Current DataFrame (e.g., inference data)</li> <li><code>alpha</code>: Significance level for statistical tests (default: 0.05)</li> </ul>"},{"location":"api/stats/#raises","title":"Raises","text":"<ul> <li><code>AssertionError</code> if drift is detected in any column</li> </ul>"},{"location":"api/stats/#low-level-statistical-tests","title":"Low-level Statistical Tests","text":""},{"location":"api/stats/#ks_test","title":"ks_test","text":"<p>Kolmogorov-Smirnov test for continuous variables.</p> <pre><code>from ml_assert.stats.drift import ks_test\n\n# Perform KS test\nstatistic, p_value = ks_test(ref_data, cur_data)\n</code></pre>"},{"location":"api/stats/#chi2_test","title":"chi2_test","text":"<p>Chi-squared test for categorical variables.</p> <pre><code>from ml_assert.stats.drift import chi2_test\n\n# Perform Chi-squared test\nstatistic, p_value = chi2_test(ref_data, cur_data)\n</code></pre>"},{"location":"api/stats/#wasserstein_distance","title":"wasserstein_distance","text":"<p>Wasserstein distance (Earth Mover's Distance) for continuous variables.</p> <pre><code>from ml_assert.stats.drift import wasserstein_distance\n\n# Calculate Wasserstein distance\ndistance = wasserstein_distance(ref_data, cur_data)\n</code></pre>"},{"location":"api/stats/#distribution-assertions","title":"Distribution Assertions","text":""},{"location":"api/stats/#assert_distribution","title":"assert_distribution","text":"<p>Assert that a dataset follows a specific distribution.</p> <pre><code>from ml_assert.stats.distribution import assert_distribution\n\n# Assert normal distribution\nassert_distribution(data, distribution=\"normal\", alpha=0.05)\n</code></pre>"},{"location":"api/stats/#parameters_1","title":"Parameters","text":"<ul> <li><code>data</code>: Array-like data to test</li> <li><code>distribution</code>: Name of the distribution to test against</li> <li><code>alpha</code>: Significance level for the test (default: 0.05)</li> </ul>"},{"location":"api/stats/#supported-distributions","title":"Supported Distributions","text":"<ul> <li>\"normal\"</li> <li>\"uniform\"</li> <li>\"exponential\"</li> <li>\"poisson\"</li> </ul>"}]}